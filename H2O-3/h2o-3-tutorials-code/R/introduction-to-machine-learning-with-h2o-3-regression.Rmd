---
title: "Regression Tutorial with H2O-3 Using R"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. To execute a code chunk, click *Run* (play) button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter.*

## Task 1: Initial Setup

```{r}
# Export the H2O library
library(h2o)

# Initialize the H2O Cluster
h2o.init(bind_to_localhost = FALSE, context_path = "h2o")
```

```{r}
# Turn off progress bars for notebook readability
h2o.no_progress()
```

```{r}
# Import dataset
loan_level <- h2o.importFile(path = "https://s3.amazonaws.com/data.h2o.ai/DAI-Tutorials/loan_level_500k.csv")
```


## Task 2: Machine Learning Concepts - See Tutorial
Please look at the tutorial file for concepts that will help you better understand this tutorial

## Task 3: Start Experiment ----

```{r}
# Print first 10 rows of the data
h2o.head(loan_level, n = 10)
```

```{r}
# Print a statistical description of the data
h2o.describe(loan_level[, c("ORIGINAL_INTEREST_RATE")])
```

```{r}
# Plot a histogram of the response column
h2o.hist(loan_level[, c("ORIGINAL_INTEREST_RATE")])
```

```{r}
# Split data into train, val
splits <- h2o.splitFrame(loan_level, c(0.7, 0.15), seed = 42)

train <- splits[[1]]
valid <- splits[[2]]
test <- splits[[3]]

dim(train)
dim(valid)
dim(test)
```

```{r}
# Identify predictors and response
ignore <- c("ORIGINAL_INTEREST_RATE", "FIRST_PAYMENT_DATE", "MATURITY_DATE", 
            "MORTGAGE_INSURANCE_PERCENTAGE", "PREPAYMENT_PENALTY_MORTGAGE_FLAG", 
            "LOAN_SEQUENCE_NUMBER", "PREPAID", "DELINQUENT", "PRODUCT_TYPE")

y <- "ORIGINAL_INTEREST_RATE"

x <- setdiff(colnames(train), ignore)
x
```

## Task 4: Build an XGBoost Model

```{r}
# Train a default XGBoost
xgb <- h2o.xgboost(x = x,
                   y = y,
                   training_frame = train,
                   model_id = "default_xgb",
                   validation_frame = valid,
                   seed = 42)

summary(xgb)
```

```{r}
# Print the variable importance of the XGBoost
h2o.varimp_plot(xgb)
```

```{r}
# Make predictions with the default model
xgb_def_pred <- h2o.predict(xgb, valid)
xgb_def_pred
```

```{r}
# Compare the predictions to the actual response
h2o.cbind(xgb_def_pred, valid[, c("ORIGINAL_INTEREST_RATE")])
```

```{r}
# Save the validation model performance of the default model
valid_def_xgb_perf <- h2o.performance(xgb, valid)
```

```{r}
# Print the validation RMSE and MAE
h2o.rmse(valid_def_xgb_perf)
h2o.mae(valid_def_xgb_perf)
```

## Task 5: Build a Deep Learning Model

```{r}
# Train a Deep Learning Model
dl <- h2o.deeplearning(x = x, 
                       y = y, 
                       training_frame = train, 
                       model_id = "default_dl",
                       validation_frame = valid, 
                       seed = 42)

dl
```

```{r}
# Plot the scoring history for the DL model
plot(dl)
```

```{r}
# Retrieve the number of epochs and hidden layers of the model
dl@allparameters[["epochs"]]
dl@allparameters[["hidden"]]
```

```{r}
# Print the variable importance
h2o.varimp_plot(dl)
```

```{r}
# Save the validation model performance
valid_def_dl_perf <- h2o.performance(dl, valid)
```

```{r}
# Print the validation RMSE and MAE of the model
h2o.rmse(valid_def_dl_perf)
h2o.mae(valid_def_dl_perf)
```

```{r}
# Make predictions on the validation set
dl_def_pred <- h2o.predict(dl, valid)
dl_def_pred
```

```{r}
# Compare the predictions to the actual response column
h2o.cbind(dl_def_pred, valid[, c("ORIGINAL_INTEREST_RATE")])
```


## Task 6: Tune the XGBoost Model with H2O GridSearch

```{r}
# Set-up the Grid Search
xgb_depth_grid <- h2o.grid(algorithm = "xgboost",
                           grid_id = "xgb_depth_grid",
                           stopping_rounds = 3,
                           stopping_metric ="RMSE",
                           stopping_tolerance = 1e-3,
                           seed = 42,
                           x = x,
                           y = y,
                           training_frame = train,
                           validation_frame = valid,
                           hyper_params = list(
                             max_depth = c(5,7,9,10,12,13,15,20)),
                           search_criteria = list(
                             strategy = "Cartesian"))
```

```{r}
# Retrieve the grid search sorted by RMSE and in ascending order
xgb_depth_grid_rmse <- h2o.getGrid(grid_id = "xgb_depth_grid", sort_by = "rmse", decreasing = FALSE)
as.data.frame(xgb_depth_grid_rmse@summary_table)
```


Do the second grid search for xgboost

```{r}
# Create sequences for three different parameters to explore more models
xgb_sample_rate <- seq(from = 0.2, to = 1, by = 0.01)
xgb_col_sample_rate <- seq(from = 0.2, to = 1, by = 0.01)
xgb_col_sample_rate_per_tree <- seq(from = 0.2, to = 1, by = 0.01)

xgb_random_grid <- h2o.grid(algorithm = "xgboost",
                            grid_id = "xgb_random_grid",
                            stopping_rounds = 3,
                            stopping_metric = "RMSE",
                            stopping_tolerance = 1e-3,
                            seed = 42,
                            ntrees = 500,
                            learn_rate = 0.25,
                            x = x,
                            y = y,
                            training_frame = train,
                            validation_frame = valid,
                            hyper_params = list(
                              max_depth = c(5,6,7,9),
                              sample_rate = xgb_sample_rate, 
                              col_sample_rate = xgb_col_sample_rate, 
                              col_sample_rate_per_tree = xgb_col_sample_rate_per_tree),
                            search_criteria = list(
                              strategy = "RandomDiscrete",
                              max_runtime_secs = 900,
                              seed = 42))
```

```{r}
# Retrieve the second Grid Search for the XGBoost
xgb_random_grid_rmse <- h2o.getGrid(grid_id = "xgb_random_grid", sort_by = "rmse", decreasing = FALSE)
as.data.frame(xgb_random_grid_rmse@summary_table)
```

```{r}
# Retrieve the best model from the Grid Search
tuned_xgb <- h2o.getModel(xgb_random_grid_rmse@model_ids[[1]]) #getting the best model
tuned_xgb
```

```{r}
# Save the validation performance of the tuned model
valid_tuned_xgb_perf <- h2o.performance(tuned_xgb, valid)
```

Print validation rmse for default and tuned model
```{r}
# Compare the RMSE of the default and tuned XGBoost
h2o.rmse(valid_def_xgb_perf)
h2o.rmse(valid_tuned_xgb_perf)
```

Print the validation mae for both default and tuned model
```{r}
# Compare the MAE of the default and tuned XGBoost
h2o.mae(valid_def_xgb_perf)
h2o.mae(valid_tuned_xgb_perf)
```


# Task 7: Tune the Deep Learning Model with H2O GridSearch

```{r}
# Do a Grid Search to tune the hidden layers and the droput ratio
dl_hidden_grid <- h2o.grid(algorithm = "deeplearning",
                           grid_id = "dl_hidden_grid",
                           activation = "RectifierWithDropout",
                           epochs = 10,
                           seed = 42,
                           stopping_rounds = 3,
                           stopping_metric ="RMSE",
                           stopping_tolerance = 1e-3,
                           x = x,
                           y = y,
                           training_frame = train,
                           validation_frame = valid,
                           hyper_params = list(
                             hidden = list(c(100, 100), c(165, 165), c(200, 200), c(330, 330),
                                           c(165, 200)),
                             hidden_dropout_ratios = list(c(0,0), c(0.01,0.01), c(0.15,0.15),
                                                          c(0.30, 0.30), c(0.5,0.5))),
                           search_criteria = list(
                             strategy = "RandomDiscrete",
                             max_runtime_secs = 900,
                             seed = 42))
```

```{r}
# Retrieve the Grid Search
dl_hidden_grid_rmse <- h2o.getGrid(grid_id = "dl_hidden_grid", sort_by = "rmse", decreasing = FALSE)
as.data.frame(dl_hidden_grid_rmse@summary_table)
```

Random grid search for DL model
```{r}
# Grid Search to tune the Max W2 and L2
dl_random_grid_rmse <- h2o.grid(algorithm = "deeplearning", 
                                grid_id = "dl_random_grid",
                                activation = "RectifierWithDropout",
                                hidden = c(165, 200),
                                epochs = 10,
                                seed = 42,
                                hidden_dropout_ratios = c(0.3, 0.3),
                                stopping_rounds = 3,
                                stopping_metric = "RMSE",
                                stopping_tolerance = 1e-3,
                                hyper_params = list(
                                  max_w2 = c(1e38, 1e35, 1e36, 1e37, 1e34, 5e35),
                                  l2 = c(1e-7, 1e-6, 1e-5, 1e-4, 5e-4, 1e-3, 0)
                                ),
                                
                                search_criteria = list(
                                  strategy = "RandomDiscrete",
                                  max_runtime_secs = 900,
                                  seed = 42
                                ),
                                
                                x = x,
                                y = y,
                                training_frame = train,
                                validation_frame = valid
)
```

```{r}
# Retrieve the Grid Search
dl_random_grid_rmse <- h2o.getGrid(grid_id = "dl_random_grid", sort_by = "rmse", decreasing = FALSE)
as.data.frame(dl_random_grid_rmse@summary_table)
```

```{r}
# Retrieve the best model from the Grid Search
tuned_dl <- h2o.getModel(dl_random_grid_rmse@model_ids[[1]]) #getting the best model
tuned_dl
```

```{r}
# Checkpointing for DL model to increase the number of epochs
dl_checkpoint <- h2o.deeplearning(x = x,
                                  y = y,
                                  training_frame = train,
                                  model_id = "dl_checkpoint",
                                  validation_frame = valid,
                                  checkpoint = dl_random_grid_rmse@model_ids[[1]],
                                  activation = "RectifierWithDropout",
                                  hidden = tuned_dl@parameters$hidden,
                                  epochs = 400,
                                  seed = 42,
                                  hidden_dropout_ratios = tuned_dl@parameters$hidden_dropout_ratios,
                                  l2 = tuned_dl@parameters$l2,
                                  max_w2 = tuned_dl@parameters$max_w2,
                                  stopping_rounds = 5,
                                  stopping_metric = "RMSE",
                                  stopping_tolerance= 1e-5)
```

```{r}
# Get the results from the previous model
dl_checkpoint
```

```{r}
# Save the validation performance of the best DL model
valid_tuned_dl_perf <- h2o.performance(dl_checkpoint, valid)
```

```{r}
# Compare the RMSE of the default and tuned DL model
h2o.rmse(valid_def_dl_perf)
h2o.rmse(valid_tuned_dl_perf)
```

```{r}
# Compare the MAE of the default and tuned DL model
h2o.mae(valid_def_dl_perf)
h2o.mae(valid_tuned_dl_perf)
```

## Task 8: Test Set Performance

```{r}
# Save the test performance of the tuned models
tuned_xgb_test_perf <- h2o.performance(tuned_xgb, test)
tuned_dl_test_perf  <- h2o.performance(dl_checkpoint, test)
```

```{r}
# Print the test RMSE of the XGB and DL model
h2o.rmse(tuned_xgb_test_perf)
h2o.rmse(tuned_dl_test_perf)
```

```{r}
# Print the test MAE of the XGB and DL model
h2o.mae(tuned_xgb_test_perf)
h2o.mae(tuned_dl_test_perf)
```

```{r}
# Make predictions with the XGB and DL model
tuned_xgb_pred <- h2o.predict(tuned_xgb, test)
tuned_dl_pred  <- h2o.predict(dl_checkpoint, test)
```

```{r}
# Compare the predictions from XGB, and DL models with the original reponse
h2o.cbind(test[, c("ORIGINAL_INTEREST_RATE")], tuned_xgb_pred, tuned_dl_pred)
```

# Challenge ----
Please look at the tutorial file for a challenge to test your understanding of this tutorial

```{r}
h2o.shutdown()
```
